{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = [] #output data: Side effect: Yes(1), No(0)\n",
    "train_samples = [] #sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example data:\n",
    "    - An experimental drug was tested on individuals from ages 13 to 65.\n",
    "    - The trial had 2100 participants. Half were under 65 years old, half were over 65 years old.\n",
    "    - 95% of patients 65 or older experienced side effects.\n",
    "    - 95% of patients under 65 experienced no side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for i in range(1000):\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 95, 24, 84, 40, 84, 22, 99, 60, 76, 27, 84, 23, 82, 57, 75, 17, 85, 57, 82, 60, 76, 27, 69, 15, 75, 25, 69, 58, 100, 19, 96, 28, 83, 14, 99, 29, 84, 15, 81, 34, 73, 52, 85, 19, 88, 39, 86, 32, 89, 28, 77, 41, 86, 44, 71, 46, 99, 14, 79, 64, 67, 46, 70, 51, 76, 56, 99, 51, 83, 39, 65, 38, 74, 36, 96, 18, 90, 56, 100, 27, 76, 27, 78, 15, 97, 62, 66, 45, 73, 40, 90, 14, 65, 47, 87, 52, 80, 56, 71, 41, 88, 25, 76, 37, 83, 50, 76, 13, 89, 31, 93, 28, 77, 63, 67, 21, 75, 17, 85, 21, 66, 44, 78, 23, 90, 19, 94, 35, 76, 58, 82, 19, 96, 46, 91, 32, 82, 18, 65, 19, 75, 37, 100, 29, 84, 39, 88, 49, 94, 20, 86, 50, 70, 53, 82, 37, 80, 44, 76, 55, 70, 60, 84, 24, 99, 35, 87, 26, 81, 49, 95, 31, 67, 56, 100, 53, 82, 35, 74, 41, 88, 31, 66, 28, 73, 49, 89, 40, 66, 32, 96, 54, 81, 63, 75, 62, 68, 45, 96, 52, 96, 63, 75, 58, 91, 57, 100, 37, 94, 54, 74, 40, 89, 46, 71, 23, 87, 58, 82, 48, 81, 60, 97, 52, 70, 58, 96, 37, 68, 57, 99, 17, 91, 31, 97, 46, 97, 57, 100, 48, 88, 49, 88, 56, 98, 44, 70, 58, 67, 34, 74, 20, 94, 39, 85, 33, 71, 39, 80, 32, 74, 18, 78, 48, 87, 59, 92, 30, 82, 62, 79, 62, 68, 56, 84, 54, 84, 53, 85, 13, 85, 28, 91, 54, 89, 26, 97, 25, 80, 25, 96, 19, 69, 55, 93, 37, 86, 39, 92, 57, 82, 57, 96, 55, 96, 55, 80, 27, 73, 24, 81, 18, 69, 55, 90, 45, 97, 53, 83, 37, 73, 42, 68, 24, 85, 46, 91, 31, 91, 45, 73, 48, 88, 29, 93, 31, 90, 51, 96, 15, 82, 64, 90, 45, 92, 13, 91, 61, 88, 21, 91, 39, 100, 53, 95, 40, 80, 24, 90, 51, 67, 32, 83, 41, 72, 45, 90, 48, 96, 34, 94, 32, 92, 44, 70, 39, 91, 48, 81, 38, 80, 60, 91, 32, 80, 62, 97, 29, 100, 14, 73, 24, 69, 40, 88, 39, 72, 46, 85, 40, 94, 59, 76, 38, 91, 27, 98, 62, 98, 62, 75, 28, 88, 14, 93, 29, 68, 49, 84, 15, 70, 28, 97, 20, 82, 40, 84, 64, 69, 36, 91, 62, 65, 41, 73, 50, 78, 25, 94, 20, 94, 18, 96, 13, 89, 16, 92, 54, 85, 38, 79, 21, 87, 53, 84, 14, 98, 18, 84, 24, 87, 22, 78, 29, 86, 60, 92, 29, 75, 54, 83, 51, 90, 55, 95, 14, 98, 23, 67, 55, 95, 15, 69, 25, 91, 61, 90, 64, 97, 29, 86, 59, 84, 25, 98, 26, 86, 21, 83, 43, 65, 61, 93, 58, 67, 18, 66, 50, 83, 15, 83, 23, 81, 38, 88, 51, 100, 38, 90, 19, 74, 59, 69, 59, 86, 40, 74, 56, 91, 63, 80, 55, 81, 28, 99, 63, 65, 64, 72, 26, 69, 64, 98, 53, 87, 43, 85, 60, 66, 37, 71, 24, 96, 34, 65, 38, 84, 60, 94, 20, 85, 18, 80, 29, 86, 52, 87, 30, 76, 23, 76, 36, 98, 18, 100, 26, 94, 25, 95, 44, 100, 46, 79, 30, 68, 29, 66, 62, 71, 57, 67, 55, 96, 14, 79, 19, 82, 51, 97, 21, 85, 23, 78, 23, 82, 62, 75, 31, 85, 52, 89, 33, 73, 20, 79, 26, 100, 31, 96, 19, 75, 27, 94, 13, 94, 48, 99, 57, 100, 35, 78, 16, 74, 48, 92, 50, 70, 54, 65, 13, 75, 43, 96, 33, 79, 13, 94, 20, 66, 13, 88, 18, 84, 34, 86, 54, 89, 27, 96, 35, 79, 33, 71, 16, 66, 49, 81, 42, 72, 44, 79, 57, 100, 56, 78, 26, 72, 31, 67, 22, 90, 56, 65, 15, 65, 21, 82, 18, 70, 41, 66, 39, 90, 20, 80, 41, 65, 14, 99, 45, 81, 24, 99, 41, 78, 51, 89, 19, 93, 21, 92, 64, 92, 48, 88, 26, 74, 40, 83, 45, 80, 57, 95, 51, 98, 16, 76, 42, 94, 51, 73, 28, 99, 36, 94, 43, 65, 47, 85, 51, 94, 49, 68, 50, 74, 29, 90, 24, 96, 25, 69, 55, 96, 18, 67, 46, 71, 63, 67, 36, 99, 62, 86, 14, 78, 31, 73, 28, 73, 58, 73, 26, 86, 23, 81, 29, 80, 33, 79, 54, 93, 42, 74, 20, 69, 54, 98, 57, 74, 21, 93, 18, 76, 50, 66, 35, 86, 55, 81, 30, 74, 14, 75, 50, 81, 48, 65, 59, 66, 53, 77, 60, 96, 56, 76, 55, 76, 27, 71, 39, 94, 16, 87, 44, 100, 51, 91, 25, 86, 39, 86, 20, 85, 54, 96, 24, 86, 54, 98, 37, 68, 38, 98, 46, 76, 56, 88, 14, 80, 19, 88, 53, 89, 14, 78, 20, 93, 19, 69, 63, 80, 29, 92, 63, 83, 56, 84, 47, 65, 39, 82, 56, 67, 48, 68, 49, 69, 17, 72, 57, 86, 46, 75, 26, 76, 26, 71, 29, 93, 38, 84, 32, 67, 17, 97, 57, 79, 58, 82, 58, 75, 16, 90, 18, 98, 47, 76, 36, 65, 22, 76, 37, 84, 57, 99, 35, 68, 42, 91, 63, 70, 50, 81, 63, 81, 42, 99, 41, 77, 29, 75, 35, 80, 57, 93, 26, 85, 28, 96, 22, 91, 60, 96, 61, 82, 25, 100, 48, 96, 37, 89, 44, 98, 41, 83, 18, 92, 44, 85, 28, 67, 42, 71, 13, 80, 31, 91, 27, 72, 47, 78, 15, 91, 56, 65, 41, 100, 44, 97, 55, 93, 13, 89, 47, 88, 54, 91, 64, 67, 26, 83, 52, 78, 36, 82, 32, 100, 58, 67, 44, 81, 49, 95, 18, 85, 38, 94, 28, 78, 24, 86, 23, 96, 47, 68, 47, 87, 24, 66, 41, 91, 60, 85, 44, 86, 29, 94, 36, 91, 50, 94, 31, 93, 20, 96, 55, 69, 28, 86, 60, 95, 16, 77, 44, 89, 18, 95, 13, 95, 13, 88, 64, 96, 13, 71, 32, 94, 54, 85, 45, 70, 46, 88, 57, 97, 43, 94, 47, 81, 43, 96, 48, 74, 31, 74, 38, 97, 14, 95, 64, 99, 45, 87, 52, 92, 47, 76, 63, 79, 16, 75, 16, 91, 23, 66, 60, 70, 43, 90, 16, 86, 28, 86, 36, 67, 43, 94, 33, 79, 29, 70, 53, 73, 28, 75, 18, 88, 58, 70, 52, 77, 33, 66, 23, 77, 51, 77, 55, 81, 35, 92, 19, 79, 48, 91, 61, 74, 40, 72, 13, 88, 17, 82, 62, 73, 51, 100, 50, 90, 62, 98, 39, 68, 17, 89, 25, 67, 19, 77, 63, 65, 32, 94, 46, 82, 24, 66, 43, 67, 13, 70, 49, 87, 30, 76, 60, 92, 25, 90, 26, 94, 31, 74, 28, 83, 41, 66, 51, 79, 19, 96, 23, 66, 43, 91, 17, 85, 27, 75, 59, 82, 62, 83, 32, 92, 26, 78, 19, 97, 59, 71, 17, 94, 56, 90, 19, 88, 64, 80, 41, 91, 62, 88, 59, 87, 29, 73, 49, 76, 16, 90, 57, 66, 23, 66, 26, 88, 14, 79, 15, 71, 22, 90, 30, 98, 18, 85, 24, 98, 45, 87, 48, 87, 13, 75, 34, 81, 23, 65, 32, 82, 14, 70, 60, 99, 16, 92, 46, 75, 60, 100, 24, 76, 59, 65, 62, 88, 57, 100, 62, 78, 58, 69, 64, 87, 54, 90, 37, 70, 56, 67, 51, 78, 49, 90, 24, 92, 17, 95, 26, 90, 35, 75, 59, 99, 15, 99, 21, 71, 30, 66, 41, 93, 25, 66, 34, 90, 27, 85, 36, 85, 19, 66, 33, 67, 28, 97, 38, 68, 54, 93, 63, 95, 64, 94, 59, 90, 31, 100, 16, 67, 61, 73, 55, 67, 36, 90, 28, 65, 32, 65, 33, 93, 64, 93, 19, 71, 51, 90, 16, 95, 31, 75, 18, 71, 21, 71, 40, 93, 52, 75, 31, 83, 37, 66, 28, 66, 21, 78, 26, 69, 13, 66, 30, 98, 43, 95, 62, 97, 19, 80, 18, 68, 56, 83, 59, 96, 15, 88, 63, 82, 15, 90, 40, 91, 39, 72, 32, 85, 38, 84, 13, 69, 53, 81, 46, 93, 20, 98, 22, 75, 35, 74, 14, 96, 49, 85, 42, 98, 57, 89, 62, 86, 49, 90, 44, 93, 22, 94, 33, 73, 56, 89, 57, 94, 17, 91, 59, 99, 43, 65, 50, 72, 24, 79, 17, 65, 38, 91, 13, 85, 48, 70, 14, 70, 26, 89, 29, 83, 19, 75, 44, 91, 54, 78, 47, 100, 16, 78, 52, 87, 35, 71, 41, 81, 29, 81, 30, 69, 47, 78, 25, 80, 13, 100, 27, 86, 43, 91, 41, 67, 21, 72, 61, 83, 33, 89, 56, 83, 33, 91, 31, 84, 32, 86, 64, 98, 42, 85, 25, 73, 46, 95, 21, 73, 25, 87, 13, 68, 39, 83, 61, 85, 30, 85, 18, 77, 34, 88, 40, 82, 37, 74, 53, 99, 57, 100, 47, 91, 53, 65, 60, 67, 62, 92, 55, 97, 35, 89, 41, 87, 15, 82, 31, 76, 29, 96, 58, 99, 59, 97, 21, 94, 52, 97, 43, 79, 16, 87, 32, 99, 40, 89, 63, 84, 14, 74, 52, 97, 35, 93, 58, 87, 55, 72, 15, 93, 16, 95, 42, 84, 44, 82, 62, 79, 62, 75, 13, 91, 49, 73, 61, 94, 29, 66, 45, 99, 41, 88, 33, 99, 60, 96, 36, 88, 45, 65, 55, 98, 46, 68, 38, 70, 28, 90, 31, 89, 50, 77, 55, 67, 36, 82, 14, 86, 18, 72, 30, 100, 28, 85, 24, 92, 45, 69, 42, 76, 41, 74, 16, 73, 58, 75, 38, 99, 49, 71, 17, 78, 13, 87, 21, 70, 26, 85, 33, 90, 60, 88, 22, 98, 53, 66, 58, 67, 30, 90, 43, 95, 30, 71, 35, 83, 22, 69, 35, 80, 38, 70, 40, 82, 26, 100, 51, 88, 43, 100, 40, 75, 42, 96, 39, 87, 41, 97, 51, 96, 25, 88, 58, 90, 51, 76, 62, 100, 16, 99, 49, 66, 59, 69, 27, 88, 57, 96, 42, 88, 35, 90, 49, 93, 36, 94, 14, 91, 55, 65, 61, 65, 57, 99, 26, 80, 24, 87, 15, 80, 17, 85, 27, 65, 43, 67, 25, 85, 23, 96, 48, 69, 23, 85, 22, 91, 38, 88, 54, 90, 62, 76, 29, 88, 17, 77, 15, 77, 59, 89, 62, 78, 39, 100, 48, 100, 42, 100, 13, 93, 25, 97, 29, 89, 19, 85, 39, 79, 52, 68, 20, 92, 54, 87, 48, 75, 48, 74, 45, 79, 51, 99, 59, 79, 30, 91, 35, 98, 40, 80, 61, 70, 50, 76, 43, 75, 48, 71, 13, 91, 20, 66, 23, 71, 19, 74, 17, 69, 36, 68, 41, 82, 20, 74, 47, 97, 28, 81, 38, 95, 56, 97, 34, 74, 35, 86, 48, 85, 45, 75, 44, 70, 31, 85, 25, 87, 16, 78, 24, 99, 47, 88, 28, 67, 47, 80, 39, 100, 43, 87, 61, 83, 48, 71, 44, 72, 43, 77, 18, 71, 33, 73, 15, 72, 16, 97, 42, 71, 54, 68, 24, 98, 46, 79, 27, 69, 33, 70, 48, 97, 58, 70, 33, 95, 22, 75, 17, 67, 37, 76, 24, 65, 28, 90, 13, 85, 18, 96, 35, 68, 47, 87, 30, 100, 40, 83, 31, 97, 28, 77, 22, 77, 58, 95, 20, 92, 41, 80, 60, 87, 35, 73, 29, 96, 55, 72, 47, 100, 28, 75, 16, 91, 62, 99, 14, 94, 37, 75, 51, 79, 43, 70, 26, 89, 37, 66, 26, 75, 41, 81, 45, 96, 14, 78, 58, 84, 38, 70, 14, 83, 43, 65, 64, 72, 57, 87, 30, 68, 25, 87, 45, 100, 32, 87, 22, 76, 60, 77, 24, 85, 21, 78, 26, 94, 24, 67, 43, 99, 38, 79, 58, 65, 41, 85, 47, 83, 20, 95, 43, 95, 43, 74, 55, 66, 32, 85, 35, 91, 34, 89, 49, 90, 47, 100, 39, 99, 46, 80, 33, 94, 13, 74, 47, 66, 16, 74, 40, 86, 59, 78, 30, 98, 54, 78, 35, 90, 51, 83, 55, 97, 34, 68, 37, 93, 55, 90, 50, 71, 28, 91, 30, 69, 40, 93, 13, 97, 32, 88, 36, 78, 16, 100]\n"
     ]
    }
   ],
   "source": [
    "#print raw data\n",
    "print(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting to array (keras needs numpy array data)\n",
    "train_lables = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Us/Public/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0,1)) #scale variable in 0s and 1s\n",
    "scaled_train_samples = scaler.fit_transform((train_samples).reshape(-1,1)) #converting data age between 16 and 100 to 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12643678],\n",
       "       [0.94252874],\n",
       "       [0.12643678],\n",
       "       ...,\n",
       "       [0.74712644],\n",
       "       [0.03448276],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print scaled data\n",
    "scaled_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sequential is a model of stack of layers\n",
    "#model = Sequential([l1, l2, l3]) #Specifying layers\n",
    "#model.add(l4) #adding a layer\n",
    "#Dense (number of nodes/output, shape of input, activation function) #A layer with its info\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape = (1,), activation = \"relu\"),\n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dense(2, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#showing summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train a keras model\n",
    "#model.compile(optimizer(learning rate), loss function/cost function, metrics to judge to accuracy )\n",
    "model.compile(Adam(lr = 0.001), loss = 'sparse_categorical_crossentropy', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a validation set\n",
    "#validation set is a part of the data that is put aside to check the accuracy of the model in future\n",
    "# Avoid overfitting\n",
    "#last percent of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1890 samples, validate on 210 samples\n",
      "Epoch 1/20\n",
      " - 0s - loss: 0.2614 - acc: 0.9354 - val_loss: 0.1101 - val_acc: 0.9952\n",
      "Epoch 2/20\n",
      " - 0s - loss: 0.2555 - acc: 0.9418 - val_loss: 0.0970 - val_acc: 0.9952\n",
      "Epoch 3/20\n",
      " - 0s - loss: 0.2560 - acc: 0.9365 - val_loss: 0.1017 - val_acc: 0.9952\n",
      "Epoch 4/20\n",
      " - 0s - loss: 0.2528 - acc: 0.9386 - val_loss: 0.1033 - val_acc: 0.9857\n",
      "Epoch 5/20\n",
      " - 0s - loss: 0.2527 - acc: 0.9386 - val_loss: 0.0994 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      " - 0s - loss: 0.2512 - acc: 0.9418 - val_loss: 0.1042 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      " - 0s - loss: 0.2517 - acc: 0.9392 - val_loss: 0.0925 - val_acc: 0.9952\n",
      "Epoch 8/20\n",
      " - 0s - loss: 0.2506 - acc: 0.9418 - val_loss: 0.0991 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      " - 0s - loss: 0.2494 - acc: 0.9386 - val_loss: 0.0919 - val_acc: 0.9952\n",
      "Epoch 10/20\n",
      " - 0s - loss: 0.2495 - acc: 0.9423 - val_loss: 0.0927 - val_acc: 0.9952\n",
      "Epoch 11/20\n",
      " - 0s - loss: 0.2477 - acc: 0.9418 - val_loss: 0.0903 - val_acc: 0.9952\n",
      "Epoch 12/20\n",
      " - 0s - loss: 0.2473 - acc: 0.9402 - val_loss: 0.0887 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      " - 0s - loss: 0.2459 - acc: 0.9444 - val_loss: 0.0919 - val_acc: 0.9857\n",
      "Epoch 14/20\n",
      " - 0s - loss: 0.2463 - acc: 0.9418 - val_loss: 0.0863 - val_acc: 0.9952\n",
      "Epoch 15/20\n",
      " - 0s - loss: 0.2453 - acc: 0.9413 - val_loss: 0.0907 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      " - 0s - loss: 0.2447 - acc: 0.9381 - val_loss: 0.0871 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      " - 0s - loss: 0.2435 - acc: 0.9423 - val_loss: 0.0848 - val_acc: 0.9952\n",
      "Epoch 18/20\n",
      " - 0s - loss: 0.2442 - acc: 0.9386 - val_loss: 0.0970 - val_acc: 0.9857\n",
      "Epoch 19/20\n",
      " - 0s - loss: 0.2423 - acc: 0.9429 - val_loss: 0.0846 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      " - 0s - loss: 0.2415 - acc: 0.9429 - val_loss: 0.0803 - val_acc: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0ee47dba8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting data\n",
    "#model.fit(samples, labels, validation set, batch_size = how many samples grow together at a time while training, epoch = iteration, suffle = Ture to make it random, verbose = how much data to print while training )\n",
    "model.fit(scaled_train_samples, train_labels, validation_split = 0.1, batch_size = 10, epochs = 20, shuffle = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "test_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "for i in range(200):\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting to array (keras needs numpy array data)\n",
    "test_lables = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Us/Public/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0,1)) #scale variable in 0s and 1s\n",
    "scaled_test_samples = scaler.fit_transform((test_samples).reshape(-1,1)) #converting data age between 16 and 100 to 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make prediction on data\n",
    "#model.predict(x,y,batch_size, verbose)\n",
    "predictions = model.predict(scaled_test_samples, batch_size = 10, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93944734 0.06055268]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.9515407  0.04845929]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.9509168  0.04908311]\n",
      "[0.00932305 0.990677  ]\n",
      "[0.9561892 0.0438108]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.95667297 0.04332704]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.0692099 0.9307901]\n",
      "[0.9516167  0.04838332]\n",
      "[0.07617144 0.92382854]\n",
      "[0.95691293 0.04308708]\n",
      "[0.00841533 0.99158466]\n",
      "[0.93944734 0.06055268]\n",
      "[0.37201884 0.6279812 ]\n",
      "[0.9564317  0.04356828]\n",
      "[0.15859275 0.84140724]\n",
      "[0.9527659  0.04723418]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.7063144 0.2936856]\n",
      "[0.37201884 0.6279812 ]\n",
      "[0.93944734 0.06055268]\n",
      "[0.11084507 0.889155  ]\n",
      "[0.89201945 0.10798059]\n",
      "[0.02849705 0.9715029 ]\n",
      "[0.95460224 0.0453978 ]\n",
      "[0.012669 0.987331]\n",
      "[0.87307084 0.12692912]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.95760727 0.04239278]\n",
      "[0.0348126  0.96518743]\n",
      "[0.95691293 0.04308708]\n",
      "[0.05702281 0.9429772 ]\n",
      "[0.93944734 0.06055268]\n",
      "[0.14528298 0.854717  ]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.95460224 0.0453978 ]\n",
      "[0.0692099 0.9307901]\n",
      "[0.9554156  0.04458447]\n",
      "[0.12144814 0.87855184]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.14528298 0.854717  ]\n",
      "[0.7063144 0.2936856]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.95483917 0.04516076]\n",
      "[0.13291384 0.8670862 ]\n",
      "[0.93944734 0.06055268]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.95184976 0.04815022]\n",
      "[0.11084507 0.889155  ]\n",
      "[0.94820476 0.05179531]\n",
      "[0.15859275 0.84140724]\n",
      "[0.9506022  0.04939787]\n",
      "[0.12144814 0.87855184]\n",
      "[0.952157   0.04784302]\n",
      "[0.05171333 0.94828665]\n",
      "[0.9502855  0.04971453]\n",
      "[0.29445466 0.7055453 ]\n",
      "[0.9502855  0.04971453]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.94435114 0.0556488 ]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.94435114 0.0556488 ]\n",
      "[0.15859275 0.84140724]\n",
      "[0.94964635 0.05035366]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.9542558  0.04574415]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.95570034 0.04429971]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.9545484  0.04545156]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.87307084 0.12692912]\n",
      "[0.0155328  0.98446727]\n",
      "[0.94820476 0.05179531]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.9076681 0.0923319]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.7734386  0.22656143]\n",
      "[0.03150206 0.96849793]\n",
      "[0.93944734 0.06055268]\n",
      "[0.00841533 0.99158466]\n",
      "[0.95483917 0.04516076]\n",
      "[0.08377025 0.9162297 ]\n",
      "[0.87307084 0.12692909]\n",
      "[0.01143927 0.98856074]\n",
      "[0.9516167  0.04838332]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.95734453 0.04265543]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.95734453 0.04265543]\n",
      "[0.15859275 0.84140724]\n",
      "[0.9578274  0.04217258]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.9502855  0.04971453]\n",
      "[0.02849705 0.9715029 ]\n",
      "[0.7063144 0.2936856]\n",
      "[0.37201884 0.6279812 ]\n",
      "[0.952157   0.04784302]\n",
      "[0.02105998 0.97893995]\n",
      "[0.9554156  0.04458447]\n",
      "[0.01402905 0.9859709 ]\n",
      "[0.9581989  0.04180107]\n",
      "[0.11084507 0.889155  ]\n",
      "[0.9527659  0.04723419]\n",
      "[0.15859275 0.84140724]\n",
      "[0.9539615  0.04603853]\n",
      "[0.04246662 0.95753336]\n",
      "[0.92124915 0.07875093]\n",
      "[0.04246662 0.95753336]\n",
      "[0.92124915 0.07875093]\n",
      "[0.13291386 0.8670861 ]\n",
      "[0.9578274  0.04217258]\n",
      "[0.0692099 0.9307901]\n",
      "[0.87307084 0.12692909]\n",
      "[0.00841533 0.99158466]\n",
      "[0.93944734 0.06055268]\n",
      "[0.10106122 0.8989388 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.10106122 0.8989388 ]\n",
      "[0.87307084 0.12692912]\n",
      "[0.05171333 0.94828665]\n",
      "[0.92124915 0.07875093]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.9506022  0.04939787]\n",
      "[0.22721158 0.77278847]\n",
      "[0.94964635 0.05035366]\n",
      "[0.07617144 0.92382854]\n",
      "[0.93298    0.06702002]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.95667297 0.04332704]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.95738596 0.04261405]\n",
      "[0.05702281 0.9429772 ]\n",
      "[0.95122975 0.04877026]\n",
      "[0.22721158 0.77278847]\n",
      "[0.93944734 0.06055268]\n",
      "[0.01903149 0.98096853]\n",
      "[0.9539615  0.04603853]\n",
      "[0.01032765 0.9896723 ]\n",
      "[0.9545484  0.04545156]\n",
      "[0.02329958 0.97670037]\n",
      "[0.9536653  0.04633472]\n",
      "[0.0348126  0.96518743]\n",
      "[0.95760727 0.04239275]\n",
      "[0.0348126  0.96518743]\n",
      "[0.89201945 0.10798059]\n",
      "[0.14528298 0.854717  ]\n",
      "[0.7734386  0.22656143]\n",
      "[0.04246662 0.95753336]\n",
      "[0.9580202  0.04197983]\n",
      "[0.01903149 0.98096853]\n",
      "[0.9536653  0.04633472]\n",
      "[0.07617144 0.92382854]\n",
      "[0.9524623  0.04753767]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.9536653  0.04633472]\n",
      "[0.0692099 0.9307901]\n",
      "[0.93944734 0.06055268]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.95738596 0.04261405]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.9530674  0.04693251]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.01903149 0.98096853]\n",
      "[0.9578274  0.04217258]\n",
      "[0.03150206 0.96849793]\n",
      "[0.87307084 0.12692912]\n",
      "[0.05702281 0.9429772 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.13291386 0.8670861 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.9502855  0.04971453]\n",
      "[0.0155328  0.98446727]\n",
      "[0.9489995  0.05100054]\n",
      "[0.04246662 0.95753336]\n",
      "[0.9524623  0.04753767]\n",
      "[0.17287514 0.8271249 ]\n",
      "[0.95715165 0.04284839]\n",
      "[0.01719494 0.9828051 ]\n",
      "[0.9524623  0.04753767]\n",
      "[0.17287514 0.8271249 ]\n",
      "[0.9564317  0.04356828]\n",
      "[0.01143927 0.98856074]\n",
      "[0.9554156  0.04458447]\n",
      "[0.17287514 0.8271249 ]\n",
      "[0.9545484  0.04545156]\n",
      "[0.01402905 0.9859709 ]\n",
      "[0.952157   0.04784302]\n",
      "[0.07617144 0.92382854]\n",
      "[0.9533673  0.04663272]\n",
      "[0.01719494 0.9828051 ]\n",
      "[0.9524623  0.04753767]\n",
      "[0.03845721 0.9615428 ]\n",
      "[0.9530674  0.04693251]\n",
      "[0.29445466 0.7055453 ]\n",
      "[0.95715165 0.04284839]\n",
      "[0.04246662 0.95753336]\n",
      "[0.82893646 0.17106356]\n",
      "[0.14528298 0.854717  ]\n",
      "[0.95734453 0.04265543]\n",
      "[0.00932305 0.990677  ]\n",
      "[0.9076681 0.0923319]\n",
      "[0.02849705 0.9715029 ]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.18816791 0.8118321 ]\n",
      "[0.9524623  0.04753767]\n",
      "[0.37201884 0.6279812 ]\n",
      "[0.7063144 0.2936856]\n",
      "[0.02105998 0.97893995]\n",
      "[0.9581989  0.04180107]\n",
      "[0.05171333 0.94828665]\n",
      "[0.9515407  0.04845929]\n",
      "[0.01402904 0.9859709 ]\n",
      "[0.95512825 0.04487173]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.952157   0.04784302]\n",
      "[0.00932305 0.990677  ]\n",
      "[0.9527659  0.04723418]\n",
      "[0.02105998 0.97893995]\n",
      "[0.94820476 0.05179531]\n",
      "[0.01032765 0.9896723 ]\n",
      "[0.9542558  0.04574415]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.92124915 0.07875093]\n",
      "[0.01143927 0.98856074]\n",
      "[0.9559454 0.0440546]\n",
      "[0.04246662 0.95753336]\n",
      "[0.89201945 0.10798059]\n",
      "[0.11084507 0.889155  ]\n",
      "[0.89201945 0.10798059]\n",
      "[0.05702281 0.9429772 ]\n",
      "[0.95691293 0.04308708]\n",
      "[0.00841533 0.99158466]\n",
      "[0.92124915 0.07875093]\n",
      "[0.02849705 0.9715029 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.12144814 0.87855184]\n",
      "[0.9559454 0.0440546]\n",
      "[0.0348126  0.96518743]\n",
      "[0.95738596 0.04261405]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.95483917 0.04516076]\n",
      "[0.02105999 0.97893995]\n",
      "[0.9076681 0.0923319]\n",
      "[0.15859275 0.84140724]\n",
      "[0.7734386  0.22656143]\n",
      "[0.15859275 0.84140724]\n",
      "[0.94932383 0.05067612]\n",
      "[0.0348126  0.96518743]\n",
      "[0.93298    0.06702002]\n",
      "[0.02105998 0.97893995]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.02849705 0.9715029 ]\n",
      "[0.87307084 0.12692912]\n",
      "[0.10106122 0.8989388 ]\n",
      "[0.95122975 0.04877026]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.9506022  0.04939787]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.9076681 0.0923319]\n",
      "[0.15859275 0.84140724]\n",
      "[0.9076681 0.0923319]\n",
      "[0.01903149 0.98096853]\n",
      "[0.95184976 0.04815022]\n",
      "[0.22721153 0.77278847]\n",
      "[0.9559454 0.0440546]\n",
      "[0.05171333 0.94828665]\n",
      "[0.7063144 0.2936856]\n",
      "[0.0692099 0.9307901]\n",
      "[0.9516167  0.04838332]\n",
      "[0.00841533 0.99158466]\n",
      "[0.9530674  0.04693253]\n",
      "[0.0692099 0.9307901]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.94932383 0.05067612]\n",
      "[0.06284127 0.9371587 ]\n",
      "[0.95483917 0.04516076]\n",
      "[0.0348126  0.96518743]\n",
      "[0.93298    0.06702002]\n",
      "[0.00841533 0.99158466]\n",
      "[0.948673   0.05132693]\n",
      "[0.17287515 0.8271249 ]\n",
      "[0.9554156  0.04458447]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.04246662 0.95753336]\n",
      "[0.94932383 0.05067612]\n",
      "[0.14528298 0.854717  ]\n",
      "[0.952157   0.04784302]\n",
      "[0.0920516  0.90794843]\n",
      "[0.9506022  0.04939787]\n",
      "[0.01903149 0.98096853]\n",
      "[0.95483917 0.04516076]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.95734453 0.04265543]\n",
      "[0.0692099 0.9307901]\n",
      "[0.9561892 0.0438108]\n",
      "[0.07617144 0.92382854]\n",
      "[0.9509168  0.04908311]\n",
      "[0.0920516  0.90794843]\n",
      "[0.95184976 0.04815023]\n",
      "[0.14528301 0.854717  ]\n",
      "[0.94820476 0.05179531]\n",
      "[0.0920516  0.90794843]\n",
      "[0.9502855  0.04971453]\n",
      "[0.01402905 0.9859709 ]\n",
      "[0.95184976 0.04815022]\n",
      "[0.22721153 0.77278847]\n",
      "[0.95122975 0.04877026]\n",
      "[0.05171333 0.94828665]\n",
      "[0.9561892 0.0438108]\n",
      "[0.01143927 0.98856074]\n",
      "[0.95734453 0.04265543]\n",
      "[0.04246662 0.95753336]\n",
      "[0.87307084 0.12692912]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.94964635 0.05035366]\n",
      "[0.00841533 0.99158466]\n",
      "[0.94820476 0.05179531]\n",
      "[0.04246662 0.95753336]\n",
      "[0.94996685 0.05003313]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.952157   0.04784302]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.01143927 0.98856074]\n",
      "[0.9536653  0.04633472]\n",
      "[0.13291386 0.8670861 ]\n",
      "[0.94964635 0.05035366]\n",
      "[0.17287514 0.8271249 ]\n",
      "[0.95738596 0.04261405]\n",
      "[0.01402904 0.9859709 ]\n",
      "[0.9502855  0.04971453]\n",
      "[0.07617144 0.92382854]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.03150206 0.96849793]\n",
      "[0.9527659  0.04723418]\n",
      "[0.01402905 0.9859709 ]\n",
      "[0.9515407  0.04845929]\n",
      "[0.0692099 0.9307901]\n",
      "[0.9554156  0.04458447]\n",
      "[0.01903149 0.98096853]\n",
      "[0.7063144 0.2936856]\n",
      "[0.00841533 0.99158466]\n",
      "[0.94964635 0.05035366]\n",
      "[0.12144814 0.87855184]\n",
      "[0.9580202  0.04197983]\n",
      "[0.01266899 0.987331  ]\n",
      "[0.95570034 0.04429971]\n",
      "[0.22721153 0.77278847]\n",
      "[0.9539615  0.04603853]\n",
      "[0.0155328  0.98446727]\n",
      "[0.952157   0.04784302]\n",
      "[0.02329958 0.97670037]\n",
      "[0.94435114 0.0556488 ]\n",
      "[0.02105998 0.97893995]\n",
      "[0.95734453 0.04265543]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.9539615  0.04603853]\n",
      "[0.14528298 0.854717  ]\n",
      "[0.93944734 0.06055266]\n",
      "[0.37201884 0.6279812 ]\n",
      "[0.87307084 0.12692912]\n",
      "[0.03150206 0.96849793]\n",
      "[0.9076681 0.0923319]\n",
      "[0.18816791 0.8118321 ]\n",
      "[0.89201945 0.10798059]\n",
      "[0.01143927 0.98856074]\n",
      "[0.9515407  0.04845929]\n",
      "[0.01402905 0.9859709 ]\n",
      "[0.77343863 0.22656138]\n",
      "[0.0348126  0.96518743]\n",
      "[0.9527659  0.04723418]\n",
      "[0.08377025 0.9162297 ]\n",
      "[0.9533673  0.04663272]\n",
      "[0.18816791 0.8118321 ]\n",
      "[0.45678622 0.5432138 ]\n",
      "[0.00841533 0.99158466]\n",
      "[0.9545484  0.04545156]\n",
      "[0.02849705 0.9715029 ]\n",
      "[0.87307084 0.12692909]\n",
      "[0.05171333 0.94828665]\n",
      "[0.94435114 0.0556488 ]\n",
      "[0.10106122 0.8989388 ]\n",
      "[0.94996685 0.05003313]\n",
      "[0.03150206 0.96849793]\n",
      "[0.9561892 0.0438108]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.9536653  0.04633472]\n",
      "[0.07617144 0.92382854]\n",
      "[0.9509168 0.0490831]\n",
      "[0.00932306 0.990677  ]\n",
      "[0.7734386  0.22656143]\n",
      "[0.08377025 0.9162297 ]\n",
      "[0.9524623  0.04753767]\n",
      "[0.18816791 0.8118321 ]\n",
      "[0.9509168  0.04908311]\n",
      "[0.05702281 0.9429772 ]\n",
      "[0.7734386  0.22656143]\n",
      "[0.29445466 0.7055453 ]\n",
      "[0.9545484  0.04545156]\n",
      "[0.02577106 0.9742289 ]\n",
      "[0.9581989  0.04180107]\n",
      "[0.0692099 0.9307901]\n",
      "[0.952157   0.04784302]\n",
      "[0.04687367 0.9531264 ]\n",
      "[0.54413295 0.45586705]\n",
      "[0.18816791 0.8118321 ]\n",
      "[0.94964635 0.05035366]\n",
      "[0.29445466 0.7055453 ]\n",
      "[0.9554156  0.04458447]\n",
      "[0.07617142 0.92382854]\n"
     ]
    }
   ],
   "source": [
    "#1 =side effect 0 =not\n",
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating rounded values\n",
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size = 10, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in rounded_predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
